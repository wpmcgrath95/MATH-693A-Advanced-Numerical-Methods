{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MATH 693A Advanced Numerical Methods: Computational Optimization HW 3\n",
    "### By Will McGrath"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 1\n",
    "### Write a Program that implements the dogleg method. Choose $B_k$ to be the exact Hessian. Apply it to solve Rosenbrock’s function: $f(x) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2$. \n",
    "### Use an initial trust region radius of 1. Set maximum trust region radius to 300. Use the initial point: $x_0 = [-1.2, 1]$ and then try another point $x_0 = [2.8, 4]$. Do the following for each of the initial points."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Here are the parameters you should use for the Dogleg Algorithm:\n",
    "### a. Use $\\| \\nabla{f(x_k)} \\|< 10^{-8}$ as the stopping criteria for your optimization algorithm.\n",
    "### b. State the total number of iterations obtained in your optimization algorithm.\n",
    "### c. Plot the objective function $f(x)$. On the same figure, plot the $x_k$ values at the different iterates of your optimization algorithm.\n",
    "### d. Plot the size of the objective function as a function of the iteration number. Use semi-log plot.\n",
    "### e. You should hand in (i) your code (ii) the first 4 and last 4 values of $x_k$ obtained from your program.\n",
    "### f. Determine the minimizer of the Rosenbrock function $x^*$.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trust-region method uses a quadratic model. At each iteration, the step is calculated by solving the following quadratic problem (sub-problem) using dogleg method:\n",
    "> Note: radius increases only if $|p_k|$ reaches the trust-region border\n",
    "- ## $\\bar{p_k} = \\argmin_{\\| \\bar{p} \\| \\le \\Delta{_k}}[f(\\bar{x_k}) + \\bar{p}^T\\nabla{f(\\bar{x_k})} + \\frac{1}{2}\\bar{p}^TB_k\\bar{p}]$\n",
    "    - ### When the first three terms of the quadratic model agrees with the Taylor expansion: S.T. $B_k = \\nabla^2f(\\bar{x_k})$, the algorithm is called the trust-region Newton Method\n",
    "### The locally constrained trust region problem is to minimize model $m_k$ whhich is based on the Taylor expansion of the objective $f$ at the current point(where $T_k$ is the trust region and $\\bar{p_k}$ is now $\\bar{p}$ since $\\bar{m_k}$ is being iterated):\n",
    "- ## $ \\bar{p_k} = \\min_{\\bar{p} \\in T_k} m_k(\\bar{p}) = \\min_{\\bar{p} \\in T_k} [f(\\bar{x_k}) + \\bar{p}^T\\nabla{f(\\bar{x_k})} + \\frac{1}{2}\\bar{p}^TB_k\\bar{p}]$\n",
    "### The full step is the unconstrained minimum of the quadratic model: \n",
    "- ## $\\bar{p_k}^{FS} = -B_k^{-1}\\nabla{f(\\bar{x_k})}$\n",
    "### The step in the steepest descent direction is given by the unconstrained minimum of the quadratic model along the steepest descent direction:\n",
    "- ## $\\bar{p_k}^{U} = - \\frac{\\nabla{f(\\bar{x_k})}^T\\nabla{f(\\bar{x_k})}}  {\\nabla{f(\\bar{x_k})}^T B_k \\nabla{f(\\bar{x_k})}} \\nabla{f(\\bar{x_k})}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Rosenbrock function\n",
    "def objective_func(xbar_k):\n",
    "    x = xbar_k[0]\n",
    "    y = xbar_k[1]\n",
    "\n",
    "    return 100*(y - x**2)**2 + (1 - x)**2\n",
    "\n",
    "def gradient(xbar_k):\n",
    "    x = xbar_k[0]\n",
    "    y = xbar_k[1]\n",
    "\n",
    "    return np.array([400*x**3 - 400*x*y + 2*x - 2, 200*(y - x**2)])\n",
    "\n",
    "def hessian(xbar_k):\n",
    "    x = xbar_k[0]\n",
    "    y = xbar_k[1]\n",
    "    \n",
    "    return np.array([[1200*x**2 - 400*y + 2, -400*x],[-400*x, 200]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dogleg Method:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def dogleg_method(grad, Hk, Bk, trust_region):\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trust Region: \n",
    "- ### Set $k = 1, \\hat{\\Delta} > 0, \\Delta_0 \\in (0, \\hat{\\Delta}),$ and $\\eta \\in (0, \\frac{1}{4})$ S.T. $\\hat{\\Delta}$ = max trust region radius, and $\\Delta_0$ = initial trust region radius\n",
    "- ### Given a step $\\bar{p_k}$ we define the ratio: $\\rho_k=\\frac{actual \\ reduction}{predicted \\ reduction} = \\frac{f(\\bar{x_k}) \\ - \\ f(\\bar{x_k} + \\bar{p_k})}{m_k(0) \\ - \\ m_k(\\bar{p_k})}$\n",
    "- ### If $\\rho_k < 0$ We shrink the size of the trust region.\n",
    "- ### If $\\rho_k ≈ 0$ Then we shrink the size of the trust region.\n",
    "- ### If $\\rho_k ≈ 1$ Then the model is in good agreement with the objective; in this case it is (probably) safe to expand the trust region for the next iteration.\n",
    "- ### Else we keep the size of the trust region."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# trust region algorithm \n",
    "def trust_reg_dogleg(x0, obj_func, grad, hess, grad_stop_criteria, eta=0.15, initial_trust_radius=1, max_trust_radius=300):\n",
    "    xbar_k = x0 # xbar_k = xbar_transposed\n",
    "    trust_reg = initial_trust_radius\n",
    "    k = 1\n",
    "\n",
    "    while np.linalg.norm(grad(xbar_k)) > grad_stop_criteria:\n",
    "        Bk = hess(xbar_k)\n",
    "        Hk = np.linalg.inv(Bk) # hessian of Bk inverse\n",
    "\n",
    "        # get approx. step pbar_k by dogleg method (gives minimized pbar_k)\n",
    "        pbar_k = dogleg_method(grad, Hk, Bk, trust_reg)\n",
    "\n",
    "        print(pbar_k)\n",
    "\n",
    "        # define a ratio measuring the success of a step\n",
    "        # given a step pbar_k we define the ratio: rho_k = actual reduction / predicted reduction \n",
    "        mk_0 = obj_func(xbar_k)\n",
    "        mk_pbar_k = obj_func(xbar_k) + np.dot(grad(xbar_k),  pbar_k) + 0.5 * np.dot(pbar_k, np.dot(Bk, pbar_k))\n",
    "        act_reduc = obj_func(xbar_k) - obj_func(xbar_k + pbar_k)\n",
    "        pred_reduc = mk_0 - mk_pbar_k\n",
    "        rho_k = act_reduc / pred_reduc\n",
    "\n",
    "        # rho is close to zero or negative, therefore the trust region must shrink\n",
    "        if rho_k < 0.25:\n",
    "            trust_reg = 0.25 * trust_reg\n",
    "\n",
    "        # rho is close to one and pbar_k has reached the boundary of the trust region, therefore the trust region must be expanded\n",
    "        # euclidean norm of pbar_k = sqrt(np.dot(pbar_k, pbar_k)) = np.linalg.norm(pbar_k)\n",
    "        else:\n",
    "            if rho_k > 0.75 and np.linalg.norm(pbar_k) == trust_reg:\n",
    "                trust_reg = min(2 * trust_reg, max_trust_radius)\n",
    "            else:\n",
    "                trust_reg = trust_reg\n",
    "\n",
    "        # choose position for the next iteration\n",
    "        if rho_k > eta:\n",
    "            xbar_k = xbar_k + pbar_k\n",
    "        else:\n",
    "            xbar_k = xbar_k\n",
    "        \n",
    "        k = k + 1\n",
    "        \n",
    "    return xbar_k"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "res = trust_reg_dogleg([-1.2, 1], objective_func, gradient, hessian, 10**(-8))\n",
    "print(\"Result of trust region dogleg method: {}\".format(res))\n",
    "print(\"Value of function at a point: {}\".format(objective_func(res)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "objective_func([-1.2, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 2\n",
    "### Experiment with the update rule for the trust region by changing the constants in Algorithm 4.1 in the text Numerical Optimization by Nocedal and Wright 2006. State what you experimented with and discuss your observations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def jumpingOnClouds(c):\n",
    "    # Write your code here\n",
    "    num_of_jumps = 0\n",
    "    indx = 0\n",
    "    while indx < len(c)-1:\n",
    "        if indx+2 < len(c) and c[indx+2] == 0:\n",
    "            indx += 2\n",
    "            num_of_jumps +=1\n",
    "\n",
    "        else:\n",
    "            indx += 1\n",
    "            num_of_jumps += 1\n",
    "\n",
    "    return num_of_jumps"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "c = [0,0,1,0,0,0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0]\n",
    "#c = [0,0]\n",
    "jumpingOnClouds(c)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "fcc29b29288bfae7cd1f9163060093e50b801b38050887fcfbbbb4f16f6170ab"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}