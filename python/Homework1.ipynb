{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "CURR_DIR = os.getcwd()\n",
    "CURR_DIR"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/Users/willmcgrath/Documents/GitHub/math_693A_class/MATH-693A-Advanced-Numerical-Methods/python'"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 1\n",
    "\n",
    "### Program the steepest descent and Newton algorithms using the backtracking line search. Use them to minimize the Rosenbrock function $f(x,y)=10(y-x^2)^2 + (1-x)^2$. \n",
    "### Initial step length $\\alpha_{0} = 1$, $\\overline{\\alpha} = 1$, $\\rho = \\frac{1}{2}$, $c = 10^{-4}$. Use initial point $\\overline{x}_{0}^{T} = [1.2, 1.2]$ and then the more difficult point $\\overline{x}_{0}^{T} = [-1.2, 1]$. \n",
    "\n",
    "### The gradient and hessian of the Rosenbrock function is: \n",
    "### $\\nabla f = \\left[\\begin{array}{c} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{array}\\right] = \\left[\\begin{array}{c} 40x^3 - 40xy +2x - 2 \\\\ 20(y-x^2) \\end{array}\\right]$,  &ensp; $\\nabla^{2} f = \\left[\\begin{array}{c} \\frac{\\partial f_{x}}{\\partial x} \\frac{\\partial f_{x}}{\\partial y} \\\\ \\frac{\\partial f_{y}}{\\partial x} \\frac{\\partial f_{y}}{\\partial y} \\end{array}\\right] = \\left[\\begin{array}{c} 120x^2 - 40y + 2 \\ -40x  \\\\ -40x \\ 20 \\end{array}\\right]$\n",
    "\n",
    "### The only minimum is at $(x,y)=(1,1)$ where $f(1,1)=0$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) Stop when: $|f(\\vec{x}_{k})| < 10^{-8}$. Print first 6 and last 6 values for  for steepest descent and Newton algorithms. Then determine the minimizer of the Rosenbrock function x*."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def test_obj_func(xbar_k):\n",
    "    x1 = xbar_k[0]\n",
    "    x2 = xbar_k[1]\n",
    "    return (x1 + x2**2)**2\n",
    "\n",
    "def test_gradient(xbar_k):\n",
    "    x1 = xbar_k[0]\n",
    "    x2 = xbar_k[1]\n",
    "    return np.array([2*(x1 + x2**2), 4*x2*(x1 + x2**2)])\n",
    "\n",
    "xbar_0_trans  = [1, 1]\n",
    "pbar_0 = - (test_gradient(xbar_0_trans)) / (np.linalg.norm(test_gradient(xbar_0_trans)))\n",
    "alpha_0 = 1\n",
    "rho = 1/2\n",
    "c = 10**-4\n",
    "\n",
    "new_alpha = alpha_0\n",
    "xbar_1 = xbar_0_trans + alpha_0*pbar_0 \n",
    "\n",
    "#first_input = xbar_0_trans.T\n",
    "#while test_obj_func()\n",
    "\n",
    "print(pbar_0)\n",
    "print(xbar_1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.4472136  -0.89442719]\n",
      "[0.5527864  0.10557281]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# need to add space in matrices above\n",
    "# functions \n",
    "def objective_func(xbar_k):\n",
    "    x = xbar_k[0]\n",
    "    y = xbar_k[1]\n",
    "\n",
    "    return 10*(y - x**2)**2 + (1 - x)**2\n",
    "\n",
    "def gradient(xbar_k):\n",
    "    x = xbar_k[0]\n",
    "    y = xbar_k[1]\n",
    "\n",
    "    return np.array([40*x**3 - 40*x*y + 2*x - 2, 20*(y - x**2)])\n",
    "\n",
    "def hessian(xbar_k):\n",
    "    x = xbar_k[0]\n",
    "    y = xbar_k[1]\n",
    "    \n",
    "    return np.array([[120*x*x - 40*y + 2, -40*x],[-40*x, 20]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "a = np.array([4, 8])\n",
    "l1 = np.linalg.norm(a)\n",
    "l1"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8.94427190999916"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "4*np.sqrt(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8.94427190999916"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# steepest descent algorithm (gradient descent using backtracking line search)\n",
    "# linear convergence\n",
    "\n",
    "def steepest_descent():\n",
    "    # use the initial step xbar_0_trans = [1.2, 1.2] \n",
    "    xbar_0_trans  = [1.2, 1.2]\n",
    "    #xbar_k = \n",
    "    #while\n",
    "\n",
    "    # find descent direction pbar_k\n",
    "    pbar_k = - (gradient(xbar_0_trans)) / (np.linalg.norm(gradient(xbar_0_trans)))\n",
    "\n",
    "    # set alpha_0 = 1, rho = 1/2, c = 10^-4, and set new_aplha = alpha_0\n",
    "    alpha_0 = 1\n",
    "    rho = 1/2\n",
    "    c = 10**-4\n",
    "    new_alpha = alpha_0\n",
    "\n",
    "    # while f(xbar_k + alpha * pbar_k) > f(xbar_k) + c * new_alpha * pbar_k_trans * f_grad(xbar_k)\n",
    "    input_val = xbar_0_trans + new_alpha * pbar_k\n",
    "    while \n",
    "\n",
    "    # set alpha_k = new_alpha"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "fcc29b29288bfae7cd1f9163060093e50b801b38050887fcfbbbb4f16f6170ab"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}