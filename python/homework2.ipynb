{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MATH 693A Advanced Numerical Methods: Computational Optimization HW 2\n",
    "### By Will McGrath"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 1 and 2\n",
    "### Use  $c = 10^{-4}$, $c_2 = 0.9$, and $ \\alpha_{max} = 1 $"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Rosenbrock function\n",
    "def objective_func(xbar_k):\n",
    "    x = xbar_k[0]\n",
    "    y = xbar_k[1]\n",
    "\n",
    "    return 100*(y - x**2)**2 + (1 - x)**2\n",
    "\n",
    "def gradient(xbar_k):\n",
    "    x = xbar_k[0]\n",
    "    y = xbar_k[1]\n",
    "\n",
    "    return np.array([400*x**3 - 400*x*y + 2*x - 2, 200*(y - x**2)])\n",
    "\n",
    "def hessian(xbar_k):\n",
    "    x = xbar_k[0]\n",
    "    y = xbar_k[1]\n",
    "    \n",
    "    return np.array([[1200*x**2 - 400*y + 2, -400*x],[-400*x, 200]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = np.arange(0, 2*np.pi+np.pi/4, 2*np.pi/8) # start, end, diff\n",
    "y = np.sin(x)\n",
    "\n",
    "plt.plot(x,y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tck = interpolate.splrep(x, y, s=0)\n",
    "xnew = np.arange(0, 2*np.pi, np.pi/50)\n",
    "ynew = interpolate.splev(xnew, tck, der=0)\n",
    "plt.plot(xnew,ynew)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Steepest Descent\n",
    "\n",
    "## $ \\bar{p_{k}}^{SD} =  -\\frac{\\nabla f(\\bar{x}_{k})}{|\\nabla f(\\bar{x}_{k})|}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# steepest descent algorithm (gradient descent using backtracking line search)\n",
    "# the backtracking algorithm only makes use of the first Wolfe condition, the Armijo condition\n",
    "# we do not have to make use of the second Wolfe condition, only condition we have to satisfy is the Armijo condition\n",
    "# linear convergence\n",
    "\n",
    "def steepest_descent(x0, objective_func, grad, val, abs_stop=True):\n",
    "    # set alpha_0 = 1, rho = 1/2, c = 10^-4\n",
    "    rho = 1/2\n",
    "    c = 10**-4\n",
    "    i = 1\n",
    "    xbar_trans = x0 # use the initial step x0\n",
    "    i_list, xbar_trans_list, obj_func_xbar_list, pbar_list, alpha_list  = [], [], [], [], []\n",
    "    if abs_stop:\n",
    "        while np.abs(objective_func(xbar_trans)) > val: \n",
    "            if i > 1:\n",
    "                xbar_trans = xbar_trans + alpha * pbar_k\n",
    "\n",
    "            # find descent direction pbar_k\n",
    "            pbar_k = - (grad(xbar_trans)) / (np.linalg.norm(grad(xbar_trans)))\n",
    "            alpha = 1\n",
    "            while(objective_func(xbar_trans + alpha * pbar_k) > objective_func(xbar_trans) + c * alpha * np.dot(pbar_k, grad(xbar_trans))):\n",
    "                alpha = rho * alpha \n",
    "\n",
    "            i_list.append(i)\n",
    "            xbar_trans_list.append(xbar_trans)\n",
    "            obj_func_xbar_list.append(objective_func(xbar_trans))\n",
    "            pbar_list.append(pbar_k)\n",
    "            alpha_list.append(alpha)\n",
    "            i += 1\n",
    "\n",
    "    else:\n",
    "        while np.linalg.norm(grad(xbar_trans)) > val: \n",
    "            if i > 1:\n",
    "                xbar_trans = xbar_trans + alpha * pbar_k\n",
    "\n",
    "            # find descent direction pbar_k\n",
    "            pbar_k = - (grad(xbar_trans)) / (np.linalg.norm(grad(xbar_trans)))\n",
    "            alpha = 1\n",
    "            while(objective_func(xbar_trans + alpha * pbar_k) > objective_func(xbar_trans) + c * alpha * np.dot(pbar_k, grad(xbar_trans))):\n",
    "                alpha = rho * alpha \n",
    "\n",
    "            i_list.append(i)\n",
    "            xbar_trans_list.append(xbar_trans)\n",
    "            obj_func_xbar_list.append(objective_func(xbar_trans))\n",
    "            pbar_list.append(pbar_k)\n",
    "            alpha_list.append(alpha)\n",
    "            i += 1\n",
    "\n",
    "    steepest_descent_df = pd.DataFrame(\n",
    "        [[i_list, xbar_trans_list, obj_func_xbar_list, pbar_list, alpha_list]], \n",
    "        columns=['iteration', 'xbar', 'f(xbar)', 'pbar', 'alpha']\n",
    "    ).explode(['iteration', 'xbar', 'f(xbar)', 'pbar', 'alpha']).reset_index(drop=True)\n",
    "\n",
    "    return steepest_descent_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Newton Method\n",
    "\n",
    "## $ \\bar{p_{k}}^{N} =  - \\begin{bmatrix} \\nabla^{2} f(\\bar{x}_{k}) \\end{bmatrix}^{-1} \\nabla f(\\bar{x}_{k}) $"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# newton method algorithm (newton method using backtracking line search)\n",
    "# the backtracking algorithm only makes use of the first Wolfe condition, the Armijo condition\n",
    "# we do not have to make use of the second Wolfe condition, only condition we have to satisfy is the Armijo condition\n",
    "# quadratic convergence\n",
    "\n",
    "def newton_method(x0, objective_func, grad, hess, val, abs_stop=True):\n",
    "    # set alpha_0 = 1, rho = 1/2, c = 10^-4\n",
    "    rho = 1/2\n",
    "    c = 10**-4\n",
    "    i = 1\n",
    "    xbar_trans = x0 # use the initial step x0\n",
    "    i_list, xbar_trans_list, obj_func_xbar_list, pbar_list, alpha_list  = [], [], [], [], []\n",
    "    if abs_stop:\n",
    "        while np.abs(objective_func(xbar_trans)) > val: \n",
    "            if i > 1:\n",
    "                xbar_trans = xbar_trans + alpha * pbar_k\n",
    "\n",
    "            # find descent direction pbar_k\n",
    "            try: \n",
    "                pbar_k = np.dot(- np.linalg.inv(hess(xbar_trans)), grad(xbar_trans))\n",
    "            except: \n",
    "                break\n",
    "                \n",
    "            alpha = 1\n",
    "            while(objective_func(xbar_trans + alpha * pbar_k) > objective_func(xbar_trans) + c * alpha * np.dot(pbar_k, grad(xbar_trans))):\n",
    "                alpha = rho * alpha \n",
    "\n",
    "            i_list.append(i)\n",
    "            xbar_trans_list.append(xbar_trans)\n",
    "            obj_func_xbar_list.append(objective_func(xbar_trans))\n",
    "            pbar_list.append(pbar_k)\n",
    "            alpha_list.append(alpha)\n",
    "            i += 1\n",
    "\n",
    "    else:\n",
    "        while np.linalg.norm(grad(xbar_trans)) > val: \n",
    "            if i > 1:\n",
    "                xbar_trans = xbar_trans + alpha * pbar_k\n",
    "\n",
    "            # find descent direction pbar_k\n",
    "            try: \n",
    "                pbar_k = np.dot(- np.linalg.inv(hess(xbar_trans)), grad(xbar_trans))\n",
    "            except: \n",
    "                break\n",
    "                \n",
    "            alpha = 1\n",
    "            while(objective_func(xbar_trans + alpha * pbar_k) > objective_func(xbar_trans) + c * alpha * np.dot(pbar_k, grad(xbar_trans))):\n",
    "                alpha = rho * alpha \n",
    "\n",
    "            i_list.append(i)\n",
    "            xbar_trans_list.append(xbar_trans)\n",
    "            obj_func_xbar_list.append(objective_func(xbar_trans))\n",
    "            pbar_list.append(pbar_k)\n",
    "            alpha_list.append(alpha)\n",
    "            i += 1\n",
    "\n",
    "    newton_method_df = pd.DataFrame(\n",
    "        [[i_list, xbar_trans_list, obj_func_xbar_list, pbar_list, alpha_list]], \n",
    "        columns=['iteration', 'xbar', 'f(xbar)', 'pbar', 'alpha']\n",
    "    ).explode(['iteration', 'xbar', 'f(xbar)', 'pbar', 'alpha']).reset_index(drop=True)\n",
    "\n",
    "    return newton_method_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def phi(obj_func, xbar_trans, alpha, pbar_k):\n",
    "    return obj_func(xbar_trans + alpha * np.array(pbar_k)) \n",
    "\n",
    "def derphi(grad, xbar_trans, alpha, pbar_k):\n",
    "    val = grad(xbar_trans + alpha * np.array(pbar_k))\n",
    "    return np.dot(val, pbar_k)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# try implementing **args\n",
    "def interp(alpha_low, alpha_high, obj_func, grad, xbar_trans, pbar_k):\n",
    "    e1 = 0.1\n",
    "    e2 = 0.1\n",
    "    alpha_km1 = alpha_low\n",
    "    alpha_k = alpha_high\n",
    "    \n",
    "    d1 = derphi(grad, xbar_trans, alpha_km1, pbar_k) + derphi(grad, xbar_trans, alpha_k, pbar_k) - \\\n",
    "        3*((phi(obj_func, xbar_trans, alpha_km1, pbar_k) - phi(obj_func, xbar_trans, alpha_k, pbar_k)) / (alpha_km1 - alpha_k))\n",
    "        \n",
    "    d2 = ((alpha_k - alpha_km1) / np.abs(alpha_k - alpha_km1)) * np.sqrt(d1**2 - derphi(grad, xbar_trans, alpha_km1, pbar_k) * derphi(grad, xbar_trans, alpha_k, pbar_k))\n",
    "\n",
    "    alpha_kp1 = alpha_k - (alpha_k - alpha_km1) * ((derphi(grad, xbar_trans, alpha_k, pbar_k) + d2 - d1) / \n",
    "                (derphi(grad, xbar_trans, alpha_k, pbar_k) - derphi(grad, xbar_trans, alpha_km1, pbar_k) + 2 * d2))\n",
    "\n",
    "    # safeguards \n",
    "    if np.abs(alpha_kp1 - alpha_k) < e1 or np.abs(alpha_kp1) < e2:\n",
    "        alpha_kp1 = alpha_k / 2\n",
    "\n",
    "    return alpha_kp1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# interpolation function\n",
    "def zoom(alpha_low, alpha_high, obj_func, grad, xbar_trans, pbar_k):\n",
    "    cnt = 0\n",
    "    c1 = 10**-4\n",
    "    c2 = 0.9 \n",
    "\n",
    "    while True:\n",
    "        # interpolate using Hermite-based cubic interpolation to find a trial step length alpha_j b/t alpha_low and alpha_high\n",
    "        alpha_j =  interp(alpha_low, alpha_high, obj_func, grad, xbar_trans, pbar_k) # (alpha_low + alpha_high) / 2\n",
    "        phi_alpha_j = phi(obj_func, xbar_trans, alpha_j, pbar_k)\n",
    "\n",
    "        # conditions\n",
    "        cond1 = phi_alpha_j > phi(obj_func, xbar_trans, 0, pbar_k) + c1 * alpha_j * derphi(grad, xbar_trans, 0, pbar_k)\n",
    "        cond2 = phi_alpha_j >= phi(obj_func, xbar_trans, alpha_low, pbar_k)\n",
    "        if cond1 or cond2:\n",
    "            alpha_high = alpha_j\n",
    "\n",
    "        else:\n",
    "            derphi_alpha_j = derphi(grad, xbar_trans, alpha_j, pbar_k)\n",
    "            if np.abs(derphi_alpha_j) <= -c2 * derphi(grad, xbar_trans, 0, pbar_k):\n",
    "                alpha_star = alpha_j\n",
    "                return alpha_star\n",
    "\n",
    "            if (derphi_alpha_j * (alpha_high - alpha_low)) >= 0: \n",
    "                alpha_high = alpha_low\n",
    "            \n",
    "            alpha_low = alpha_j\n",
    "       \n",
    "        cnt += 1\n",
    "        print(f'Count: {cnt}')\n",
    "        print(f'alpha_j: {alpha_j}')\n",
    "        print(f'alpha_high: {alpha_high}')\n",
    "        print(f'alpha_low: {alpha_low}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "zoom(1, 1.2, objective_func, gradient, [1.2, 1.2], [-1,1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# LS/Strong Wolfe Conditions algorithm\n",
    "def strong_wolfe_conditions(obj_func, grad, xbar_trans, alpha, pbar_k):\n",
    "    alpha_0 = 0\n",
    "    alpha = 0.1\n",
    "    alpha_max = 1\n",
    "    c1 = 10**-4\n",
    "    c2 = 0.9 \n",
    "\n",
    "    while True:\n",
    "        phi_alpha_i = phi(obj_func, xbar_trans, alpha, pbar_k)\n",
    "        cond1 = (phi_alpha_i > phi(obj_func, xbar_trans, 0, pbar_k) + c1 * alpha * derphi(grad, xbar_trans, 0, pbar_k))\n",
    "        cond2 = ((phi_alpha_i >= phi(obj_func, xbar_trans, alpha, pbar_k)) and i > 1)\n",
    "        if cond1 or cond2: \n",
    "            alpha_star = zoom(alpha, alpha, obj_func, grad, xbar_trans, pbar_k)\n",
    "            return alpha_star\n",
    "\n",
    "    alpha = alpha\n",
    "    alpha_1 = (alpha_1 + alpha_max) / 2\n",
    "    alpha_ip1 = random.random(alpha*100, alpha_max*100) / 100\n",
    "\n",
    "    return "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 3\n",
    "### You should state in your HW how you computed $\\alpha_{i+1}$ on Line 11 of the LS/Strong Wolfe Conditions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "=> I computed $\\alpha_{i+1}$ by randomly choosing a value in [$\\alpha_{i}$, $\\alpha_{max}$]. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 4\n",
    "### You should state in your HW the Interpolation method used"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "=> I used the Hermite-based cubic interpolation method."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 5 \n",
    "### You should state in your HW the value of epsilon used"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "=> The epsilons are located in the safeguards conditions, which are located in the interpolation function. I used $\\epsilon_1 = \\epsilon_2 = 0.1 $. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 6\n",
    "### Plot the objective function $f(x)$. On the same figure, plot the $x_k$ values at different iterates"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sd_df = steepest_descent([1.2, 1.2], objective_func, gradient, 10**-8)\n",
    "sd_xbar_list = sd_df['xbar'].to_list()\n",
    "sd_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nm_df = newton_method([1.2, 1.2], objective_func, gradient, hessian, 10**-8)\n",
    "nm_xbar_list = nm_df['xbar'].to_list()\n",
    "nm_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def contourplot(objective_func, x_range, y_range, title, ncontours=500):\n",
    "    xmin = x_range[0]\n",
    "    xmax = x_range[1]\n",
    "    ymin = y_range[0]\n",
    "    ymax = y_range[1]\n",
    "\n",
    "    # range of x and y \n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    y = np.linspace(ymin, ymax, 100)\n",
    "    X, Y = np.meshgrid(x,y)\n",
    "    xbar = [X,Y]\n",
    "    Z = objective_func(xbar)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.contour(X, Y, Z, ncontours, cmap = 'hsv'); # plot the contours\n",
    "    plt.scatter(1, 1, marker=\"x\", s=75, color=\"black\", label = 'Minimum');  # mark the minimum\n",
    "    plt.legend(loc=0)\n",
    "    plt.title(\"Minimize $f(x,y)=100(y-x^2)^2 + (1-x)^2$ Using %s\"%title);\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot objective function\n",
    "contourplot(objective_func, [0.8, 1.3], [0.7, 1.3], \"Steepest Descent\")\n",
    "\n",
    "# plot random iterations\n",
    "pt1 = sd_xbar_list[0]\n",
    "pt2 = sd_xbar_list[300]\n",
    "pt3 = sd_xbar_list[700]\n",
    "pt4 = sd_xbar_list[1500]\n",
    "pt5 = sd_xbar_list[3000]\n",
    "pt6 = sd_xbar_list[-1]\n",
    "all_pts = [pt1, pt2, pt3, pt4, pt5, pt6]\n",
    "\n",
    "for pt in all_pts:\n",
    "    plt.scatter(pt[0], pt[1], marker='o', s=10, color='black');\n",
    "\n",
    "for i in range(1, len(all_pts)):\n",
    "    plt.plot((all_pts[i-1][0], all_pts[i][0]), (all_pts[i-1][1], all_pts[i][1]), color='black');"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot objective function\n",
    "contourplot(objective_func, [0.8, 1.3], [0.7, 1.5], \"Newton's Method\")\n",
    "\n",
    "# plot random iterations\n",
    "for pt in nm_xbar_list:\n",
    "    plt.scatter(pt[0], pt[1], marker='o', s=10, color='black');\n",
    "\n",
    "for i in range(1, len(nm_xbar_list)):\n",
    "    plt.plot((nm_xbar_list[i-1][0], nm_xbar_list[i][0]), (nm_xbar_list[i-1][1], nm_xbar_list[i][1]), color='black');"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem 7\n",
    "### Plot the size of the objective function as a function of the iteration number. Use semi-log plot."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# steepest descent and newton's method semi-log plots\n",
    "def plot_semilogy(sd_f_xbar_list, nm_f_xbar_list):\n",
    "    sd_indx_list = np.linspace(1, len(sd_f_xbar_list), 15)\n",
    "    nm_indx_list = np.linspace(1, len(nm_f_xbar_list), len(nm_f_xbar_list))\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.semilogy(sd_indx_list, sd_f_xbar_list, label='Steepest Descent');\n",
    "    plt.semilogy(nm_indx_list, nm_f_xbar_list, label=\"Newton's Method\");\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Log of f(xbar)')\n",
    "    plt.title('Algorithm Convergence')\n",
    "    plt.xticks(np.arange(0,16))\n",
    "    plt.legend(loc=0)\n",
    "\n",
    "    return None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.linspace(sd_df['f(xbar)'].to_list()[0], sd_df['f(xbar)'].to_list()[-1], 15)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot\n",
    "# we see that the Steepest Descent algorithm is linearly convergent \n",
    "# Newton algorithm is significantly faster (quadratically convergent)\n",
    "sd_f_xbar_list = sd_df['f(xbar)'].to_list()[0:15]\n",
    "nm_f_xbar_list = nm_df['f(xbar)'].to_list()\n",
    "plot_semilogy(sd_f_xbar_list, nm_f_xbar_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 8\n",
    "### Compare the performance for both the Newton and Steepest Descent algorithms; is there a significant difference in number of iterations etc.? Discuss this."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "=> The number of iterations for Newton's Method is significantly less than the number of iterations in Steepest Descent. As we can see in the plot above, Steepest Descent converges linearly (constant in a small window) and Newton's Method converges to the minimum quadratically. Newton's Method only required 8 iterations while Steepest Descent required 7612 iterations. "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcc29b29288bfae7cd1f9163060093e50b801b38050887fcfbbbb4f16f6170ab"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}