{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MATH 693A Advanced Numerical Methods: Computational Optimization HW 4\n",
    "###  By: Will McGrath\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 1 \n",
    "### Program that trust-region method using the Steihaug method (see Lecture 15 and/or Algorithm 7.2 in the text Numerical Optimization by Nocedal and Wright 2006). Choose $B_k$ to be the exact Hessian, and use it to minimize the function: $f(x) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2$. \n",
    "\n",
    "### Use the tolerance $\\epsilon = 10^{−6}$. $\\| \\nabla{f(x_k)} \\|< 10^{-8}$ as the stopping criteria for your optimization algorithm. Use an initial trust region radius of 1. Set maximum trust region radius to 300. Use the initial point: $x_0 = [-1.2, 1]$ and then try another point $x_0 = [2.8, 4]$. Do the following for each of the initial points."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Rosenbrock function\n",
    "def objective_func(xbar_k):\n",
    "    x = xbar_k[0]\n",
    "    y = xbar_k[1]\n",
    "\n",
    "    return 100*(y - x**2)**2 + (1 - x)**2\n",
    "\n",
    "def gradient(xbar_k):\n",
    "    x = xbar_k[0]\n",
    "    y = xbar_k[1]\n",
    "\n",
    "    return np.array([400*x**3 - 400*x*y + 2*x - 2, 200*(y - x**2)])\n",
    "\n",
    "def hessian(xbar_k):\n",
    "    x = xbar_k[0]\n",
    "    y = xbar_k[1]\n",
    "    \n",
    "    return np.array([[1200*x**2 - 400*y + 2, -400*x],[-400*x, 200]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part A\n",
    "\n",
    "### Your program should indicate, at every iteration, whether Steihaug method encountered negative curvature, reached the trust-region boundary, or met the stopping test. Hand in your output.\n",
    "> Note: Use Steihaug method to find pk and feed it into tr method\n",
    "\n",
    "> Note: Quasi-Newton = just a different, modified hessian but methods are similar. The hessian doesn't have to be positive definite bc you can edit it to make it positive def or use a method that doesn't require it"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tau Star"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# compute tau star s.t. pbar_k = ||pbar_k + tau*dbar_k|| = trust_region\n",
    "# simplifies to quadratic equation\n",
    "def compute_tau_star(pbar_k, dbar_k, trust_reg):\n",
    "    a_coeff = np.dot(dbar_k, dbar_k)\n",
    "    b_coeff = 2 * np.dot(dbar_k, pbar_k)\n",
    "    c_coeff = np.dot(pbar_k, pbar_k) - trust_reg**2\n",
    "\n",
    "    # solve quadratic equation\n",
    "    tau_1 = (-b_coeff + np.sqrt(b_coeff * b_coeff - 4 * a_coeff * c_coeff)) / (2 * a_coeff) \n",
    "    tau_2 = (-b_coeff - np.sqrt(b_coeff * b_coeff - 4 * a_coeff * c_coeff)) / (2 * a_coeff) \n",
    "\n",
    "    # conditions to choose tau star\n",
    "    if tau_1 >= 0 and tau_2 < 0:\n",
    "        return tau_1\n",
    "\n",
    "    if tau_2 >= 0 and tau_1 < 0:\n",
    "        return tau_2\n",
    "\n",
    "    if tau_1 >= 0 and tau_2 >= 0:\n",
    "        return min(tau_1, tau_2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Steihaug Method\n",
    "- Quadratic convergence (i.e. $\\epsilon = 10^{-6} \\sim \\| \\nabla{f(x_k)} \\|$ )"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# trust_reg = delta\n",
    "# eps = tolerance\n",
    "# don't need Hk (Bk inverse or hessian inverse)\n",
    "def steihaug_method(xbar_k, grad, Bk, trust_reg, eps=10**(-6)):\n",
    "    pbar_k = [0, 0]\n",
    "    rbar_k = grad(xbar_k) # residuals\n",
    "    dbar_k = -rbar_k\n",
    "\n",
    "    if np.linalg.norm(rbar_k) < eps:\n",
    "        return pbar_k \n",
    "\n",
    "    while True:\n",
    "        # negative curvature\n",
    "        if np.dot(dbar_k, np.dot(Bk, dbar_k)) <= 0:\n",
    "            tau_star = compute_tau_star(pbar_k, dbar_k, trust_reg)\n",
    "            pbar_k = pbar_k + tau_star * dbar_k\n",
    "            print('Negative curvature')\n",
    "            return pbar_k\n",
    "        \n",
    "        alpha_k = np.dot(rbar_k, rbar_k) / np.dot(dbar_k, np.dot(Bk, dbar_k))\n",
    "        pbar_kp1 = pbar_k + alpha_k * dbar_k\n",
    "\n",
    "        # step outside trust region\n",
    "        if np.linalg.norm(pbar_kp1) >= trust_reg:\n",
    "            tau_star = compute_tau_star(pbar_k, dbar_k, trust_reg)\n",
    "            pbar_k = pbar_k + tau_star * dbar_k\n",
    "            print('Reached the trust region boundary')\n",
    "            return pbar_k\n",
    "\n",
    "        # stopping criteria\n",
    "        rbar_kp1 = rbar_k + alpha_k * np.dot(Bk, dbar_k)\n",
    "        if np.linalg.norm(rbar_kp1) <= (eps * np.linalg.norm(grad(xbar_k))): #(eps * np.linalg.norm(rbar_k)):\n",
    "            print('Met stopping criteria')\n",
    "            return pbar_kp1\n",
    "        \n",
    "        # find Bk+1\n",
    "        Bkp1 = np.dot(rbar_kp1, rbar_kp1) / np.dot(rbar_k, rbar_k)\n",
    "        dbar_kp1 = -rbar_kp1 + np.dot(Bkp1, dbar_k)\n",
    "\n",
    "        # set next iteration\n",
    "        Bk = Bkp1\n",
    "        dbar_k = dbar_kp1\n",
    "        rbar_k = rbar_kp1\n",
    "        pbar_k = pbar_kp1\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trust Region\n",
    "- Set $k = 1, \\hat{\\Delta} > 0, \\Delta_0 \\in (0, \\hat{\\Delta}),$ and $\\eta \\in (0, \\frac{1}{4})$ S.T. $\\hat{\\Delta}$ = max trust region radius, and $\\Delta_0$ = initial trust region radius\n",
    "- Given a step $\\bar{p_k}$ we define the ratio: $\\rho_k=\\frac{actual \\ reduction}{predicted \\ reduction} = \\frac{f(\\bar{x_k}) \\ - \\ f(\\bar{x_k} + \\bar{p_k})}{m_k(0) \\ - \\ m_k(\\bar{p_k})}$\n",
    "- If $\\rho_k < 0$ We shrink the size of the trust region.\n",
    "- If $\\rho_k ≈ 0$ Then we shrink the size of the trust region.\n",
    "- If $\\rho_k ≈ 1$ Then the model is in good agreement with the objective; in this case it is (probably) safe to expand the trust region for the next iteration.\n",
    "- Else we keep the size of the trust region."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# trust region algorithm \n",
    "def trust_region(x0, obj_func, grad, hess, grad_stop_criteria, eta=0.15, initial_trust_radius=1, max_trust_radius=30):\n",
    "    xbar_k = x0 # xbar_k = xbar_transposed\n",
    "    trust_reg = initial_trust_radius\n",
    "    k = 1\n",
    "    k_list, xbar_list, obj_func_xbar_list, pbar_list, rho_list, trust_reg_list = [], [], [], [], [], []\n",
    "\n",
    "    while np.linalg.norm(grad(xbar_k)) > grad_stop_criteria:\n",
    "        Bk = hess(xbar_k)\n",
    "        \n",
    "        # get approx. step pbar_k by steihaug method (gives minimized pbar_k)\n",
    "        pbar_k = steihaug_method(xbar_k, grad, Bk, trust_reg)\n",
    "        print(pbar_k)\n",
    "\n",
    "        # define a ratio measuring the success of a step\n",
    "        # given a step pbar_k we define the ratio: rho_k = actual reduction / predicted reduction \n",
    "        mk_0 = obj_func(xbar_k)\n",
    "        mk_pbar_k = obj_func(xbar_k) + np.dot(grad(xbar_k),  pbar_k) + 0.5 * np.dot(pbar_k, np.dot(Bk, pbar_k))\n",
    "        act_reduc = obj_func(xbar_k) - obj_func(xbar_k + pbar_k)\n",
    "        pred_reduc = mk_0 - mk_pbar_k\n",
    "        rho_k = act_reduc / pred_reduc\n",
    "\n",
    "        # rho is close to zero or negative, therefore the trust region must shrink\n",
    "        if rho_k < 0.25:\n",
    "            trust_reg = 0.25 * trust_reg\n",
    "\n",
    "        # rho is close to one and pbar_k has reached the boundary of the trust region, therefore the trust region must be expanded\n",
    "        # euclidean norm of pbar_k = sqrt(np.dot(pbar_k, pbar_k)) = np.linalg.norm(pbar_k)\n",
    "        else:\n",
    "            if rho_k > 0.75 and np.linalg.norm(pbar_k) == trust_reg:\n",
    "                trust_reg = min(2 * trust_reg, max_trust_radius)\n",
    "            else:\n",
    "                trust_reg = trust_reg\n",
    "        \n",
    "        # add to dataframe\n",
    "        if k == 1:\n",
    "            xbar_list.append(x0)\n",
    "            k_list.append(0)\n",
    "            obj_func_xbar_list.append(obj_func(x0))\n",
    "            pbar_list.append(np.nan)\n",
    "            rho_list.append(np.nan)\n",
    "            trust_reg_list.append(np.nan)\n",
    "\n",
    "        # choose position for the next iteration\n",
    "        if rho_k > eta:\n",
    "            xbar_k = xbar_k + pbar_k\n",
    "        else:\n",
    "            xbar_k = xbar_k\n",
    "        \n",
    "        xbar_list.append(xbar_k)\n",
    "        k_list.append(k)\n",
    "        obj_func_xbar_list.append(obj_func(xbar_k))\n",
    "        pbar_list.append(pbar_k)\n",
    "        rho_list.append(rho_k)\n",
    "        trust_reg_list.append(trust_reg)\n",
    "\n",
    "        k = k + 1\n",
    "\n",
    "        trust_region_steihaug_df = pd.DataFrame(\n",
    "        [[k_list, xbar_list, obj_func_xbar_list, pbar_list, rho_list, trust_reg_list]], \n",
    "        columns=['iteration', 'xbar', 'f(xbar)', 'pbar', 'rho', 'trust_region']\n",
    "    ).explode(['iteration', 'xbar', 'f(xbar)', 'pbar', 'rho', 'trust_region']).reset_index(drop=True)\n",
    "\n",
    "    return trust_region_steihaug_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# [-1.2, 1] = initial point in trust region\n",
    "trust_region_steihaug_df = trust_region([-1.2, 1], objective_func, gradient, hessian, 10**(-2))\n",
    "trust_region_steihaug_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part B\n",
    "\n",
    "### State the total number of iterations obtained in your optimization algorithm."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The total number of iteration was 423. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part C\n",
    "\n",
    "### Plot the objective function $f(x)$. On the same figure, plot the $x_k$ values at the different iterates of your optimization algorithm."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def contourplot(objective_func, x_range, y_range, title, ncontours=500):\n",
    "    xmin = x_range[0]\n",
    "    xmax = x_range[1]\n",
    "    ymin = y_range[0]\n",
    "    ymax = y_range[1]\n",
    "\n",
    "    # range of x and y \n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    y = np.linspace(ymin, ymax, 100)\n",
    "    X, Y = np.meshgrid(x,y)\n",
    "    xbar = [X,Y]\n",
    "    Z = objective_func(xbar)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.contour(X, Y, Z, ncontours, cmap = 'hsv'); # plot the contours\n",
    "    plt.scatter(1, 1, marker=\"x\", s=150, color=\"black\", label = 'Minimum');  # mark the minimum\n",
    "    plt.legend(loc=0)\n",
    "    plt.title(\"Minimize $f(x,y)=100(y-x^2)^2 + (1-x)^2$ with %s\"%title);\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot objective function\n",
    "contourplot(objective_func, [-1.4, 1.4], [-0.3, 1.3], \"x0 = [-1.2, 1] Using Trust Region Method With Steihaug Method\")\n",
    "\n",
    "# plot random iterations\n",
    "# xbar_list_1 = trust_region_steihaug_df['xbar'].to_list()\n",
    "# pt1 = xbar_list_1[0]\n",
    "# pt2 = xbar_list_1[5]\n",
    "# pt3 = xbar_list_1[10]\n",
    "# pt4 = xbar_list_1[15]\n",
    "# pt5 = xbar_list_1[20]\n",
    "# pt6 = xbar_list_1[25]\n",
    "# all_pts = [pt1, pt2, pt3, pt4, pt5, pt6]\n",
    "\n",
    "# for pt in all_pts:\n",
    "#     plt.scatter(pt[0], pt[1], marker='o', s=10, color='black');\n",
    "\n",
    "# for i in range(1, len(all_pts)):\n",
    "#     plt.plot((all_pts[i-1][0], all_pts[i][0]), (all_pts[i-1][1], all_pts[i][1]), color='black');"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot objective function\n",
    "contourplot(objective_func, [-1.3, 3.1], [-0.4, 4.5], \"x0 = [2.8, 4] Using Trust Region Method With Steihaug Method\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part D\n",
    "\n",
    "### Plot the size of the objective function as a function of the iteration number. Use semi-log plot."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part E\n",
    "\n",
    "### Determine the minimizer of the function $x^*$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The minimizer for the function: $f(x) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2$ is $x^* $ where $x^* = [1, 1]$. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 2\n",
    "\n",
    "### Implement the standard CG algorithm, and use it to solve linear systems in which A is the Hilbert matrix, whose elements are $\\alpha_{ij} = 1 / (i + j - 1)$. Set the right-hand-side to be all ones $\\vec{b} = ones(n,1)$, and the initial point to be the origin $\\vec{x_o} = zeros(n,1)$. In the stopping criteria, use $\\| \\nabla{r_k)} \\|> 10^{-6}$. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# congujate gradient method\n",
    "def linear_conjugate_gradient(A, b, x0, stopping_criteria = 10**(-6)):\n",
    "    xk = x0\n",
    "    rk = np.dot(A, xk) - b # residuals\n",
    "    pk = -rk\n",
    "    k = 0 # count \n",
    "    xk_list, rk_norm_list, residual_list, iteration_list = [], [], [], []\n",
    "\n",
    "    while np.linalg.norm(rk) > stopping_criteria:\n",
    "        \n",
    "        ap_k = np.dot(A, pk) \n",
    "        rk_rk = np.dot(rk, rk)\n",
    "        alpha = rk_rk / np.dot(pk, ap_k)\n",
    "\n",
    "        if k == 0:\n",
    "            iteration_list.append(k)\n",
    "            residual_list.append(rk)\n",
    "            xk_list.append(xk)\n",
    "            rk_norm_list.append(np.linalg.norm(rk))\n",
    "\n",
    "        xk = xk + alpha * pk\n",
    "        rk = rk + alpha * ap_k\n",
    "        beta = np.dot(rk, rk) / rk_rk\n",
    "        pk = -rk + beta * pk\n",
    "\n",
    "        k += 1\n",
    "        iteration_list.append(k)\n",
    "        residual_list.append(rk)\n",
    "        xk_list.append(xk)\n",
    "        rk_norm_list.append(np.linalg.norm(rk))\n",
    "\n",
    "    df = pd.DataFrame([[iteration_list, xk_list, residual_list, rk_norm_list]], \n",
    "        columns=['iteration', 'xbar', 'residuals', 'norm_residuals']\n",
    "    ).explode(['iteration', 'xbar', 'residuals', 'norm_residuals']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# hilbert matrix\n",
    "def hilbert(n):\n",
    "    matrix = []\n",
    "    for i in range(1, n + 1):\n",
    "        row = []\n",
    "        for j in range(1, n + 1):\n",
    "            row.append(1 / (i + j - 1))\n",
    "        matrix.append(row)\n",
    "        \n",
    "    return matrix\n",
    "\n",
    "def bn(n):\n",
    "    return np.ones(n)\n",
    "\n",
    "def xn(n):\n",
    "    return np.zeros(n)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "linear_conjugate_gradient(hilbert(5), bn(5), xn(5))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part A\n",
    "\n",
    "### For dimensions n = 5, 8, 12, 20, plot the norm of the residual as a function of the iteration (on the same figure); stop when the norm is less than $10^{-6}$. Use semi-log plot."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n5_df = linear_conjugate_gradient(hilbert(5), bn(5), xn(5))\n",
    "n8_df = linear_conjugate_gradient(hilbert(8), bn(8), xn(8))\n",
    "n12_df = linear_conjugate_gradient(hilbert(12), bn(12), xn(12))\n",
    "n20_df = linear_conjugate_gradient(hilbert(20), bn(20), xn(20))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(8,7))\n",
    "plt.plot(n5_df['iteration'].to_list(), n5_df['norm_residuals'].to_list(), label = 'n = 5')\n",
    "plt.plot(n8_df['iteration'].to_list(), n8_df['norm_residuals'].to_list(), label = 'n = 8')\n",
    "plt.plot(n12_df['iteration'].to_list(), n12_df['norm_residuals'].to_list(), label = 'n = 12')\n",
    "plt.plot(n20_df['iteration'].to_list(), n20_df['norm_residuals'].to_list(), label = 'n = 20')\n",
    "plt.legend(loc=0)\n",
    "plt.title('Norm of the Residual as a Function of the Iteration')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Residual Norm')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(8,7))\n",
    "plt.semilogy(n5_df['iteration'].to_list(), n5_df['norm_residuals'].to_list(), label = 'n = 5')\n",
    "plt.semilogy(n8_df['iteration'].to_list(), n8_df['norm_residuals'].to_list(), label = 'n = 8')\n",
    "plt.semilogy(n12_df['iteration'].to_list(), n12_df['norm_residuals'].to_list(), label = 'n = 12')\n",
    "plt.semilogy(n20_df['iteration'].to_list(), n20_df['norm_residuals'].to_list(), label = 'n = 20')\n",
    "plt.legend(loc=0)\n",
    "plt.title('Semi-Log of Residual Norm as a Function of the Iteration Using Semi-Log')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Semi-Log of Residual Norm')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(8,7))\n",
    "plt.plot(n5_df['iteration'].to_list(), np.log(n5_df['norm_residuals'].to_list()), label = 'n = 5')\n",
    "plt.plot(n8_df['iteration'].to_list(), np.log(n8_df['norm_residuals'].to_list()), label = 'n = 8')\n",
    "plt.plot(n12_df['iteration'].to_list(), np.log(n12_df['norm_residuals'].to_list()), label = 'n = 12')\n",
    "plt.plot(n20_df['iteration'].to_list(), np.log(n20_df['norm_residuals'].to_list()), label = 'n = 20')\n",
    "plt.legend(loc=0)\n",
    "plt.title('Log of Residual Norm as a Function of the Iteration')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log of Residual Norm')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part B\n",
    "\n",
    "### Plot your number of iterations against n for n = 5, 8, 12, 20."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(8,7))\n",
    "max_iteration_list = [max(n5_df['iteration'].to_list()), max(n8_df['iteration'].to_list()),\n",
    "                     max(n12_df['iteration'].to_list()), max(n20_df['iteration'].to_list())]\n",
    "                     \n",
    "plt.plot([5, 8, 12, 20], max_iteration_list)\n",
    "plt.title('Number of Iterations Against n')\n",
    "plt.xticks(np.arange(5,21,1))\n",
    "plt.xlabel('n')\n",
    "plt.yticks(np.arange(5,80,5))\n",
    "plt.ylabel('Iteration')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part C\n",
    "\n",
    "### Compute the condition number for your Hilbert matrices, generate a plot of the condition number against the matrix size n. Use semi-log plot.\n",
    ">Note: The Hilbert matrix shows up in the normal equations in least squares approximations and is an example of a matrix with a nasty condition number. Note that the Hilbert matrix is a square matrix, therefore a matrix size n denotes an 𝑛 × 𝑛 matrix.\n",
    "\n",
    "\n",
    ">Note: The condition number of x is defined as the norm of x times the norm of the inverse of x [1]; the norm can be the usual L2-norm (root-of-sum-of-squares) or one of a number of other matrix norms."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hilbert_matrix_list = [hilbert(n) for n in [5, 8, 12, 20]]\n",
    "condition_list = [np.linalg.cond(a) for a in hilbert_matrix_list]\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "plt.semilogy([5, 8, 12, 20], condition_list)\n",
    "plt.title('Semi-Log of Condition Number Against the Matrix Size n')\n",
    "plt.xticks(np.arange(5,21,1))\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('Semi-Log of Condition Number')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part D\n",
    "\n",
    "### Plot the eigenvalues for n = 5, 8, 12, 20 on the same figure in order to show the spread of the eigenvalues.\n",
    ">Note: An eigenvalue is a number, telling you how much variance there is in the data in that direction, in the example above the eigenvalue is a number telling us how spread out the data is on the line. The eigenvector with the highest eigenvalue is therefore the principal component. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "eigenvalue_list = [np.linalg.eig(e) for e in hilbert_matrix_list]\n",
    "ones_list = [np.ones(n) * n for n in [5, 8, 12, 20]]\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "for i in range(len([5, 8, 12, 20])):\n",
    "    plt.plot(ones_list[i], eigenvalue_list[i][0], marker='o')\n",
    "plt.legend(['n = 5', 'n = 8', 'n = 12', 'n = 20'], loc=0);\n",
    "plt.title('Eigenvalues Against the Matrix Size n')\n",
    "plt.xticks(np.arange(5,21,1))\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('Eigenvalues')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(8,7))\n",
    "for i in range(len([5, 8, 12, 20])):\n",
    "    plt.plot(ones_list[i], np.log(eigenvalue_list[i][0]), marker='o')\n",
    "plt.legend(['n = 5', 'n = 8', 'n = 12', 'n = 20'], loc=0);\n",
    "plt.title('Log of Eigenvalues Against the Matrix Size n')\n",
    "plt.xticks(np.arange(5,21,1))\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('Log of Eigenvalues')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part E\n",
    "\n",
    "### From the formulas provided in the Lecture 11, estimate how many steepest descent iterations you would need to solve the problem to the same precision. What can you say about your estimate, is it a good estimate?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### For the Conjugate Gradient (CG) method: $\\vert\\vert \\bar{x_k} - \\bar{x}^* \\vert\\vert_A \\le 2\\begin{bmatrix} \\frac{\\sqrt{K(A)} - 1}{\\sqrt{K(A)} + 1} \\end{bmatrix}^k \\vert\\vert \\bar{x_0} - \\bar{x}^* \\vert\\vert_A$, where $K(A) = \\frac{\\lambda_n}{\\lambda_1} $. For Steepest Descent (SD) method: $\\vert\\vert \\bar{x_{k+1}} - \\bar{x}^* \\vert\\vert_A \\le 2\\begin{bmatrix} \\frac{K(A) - 1}{K(A)+ 1} \\end{bmatrix}^k \\vert\\vert \\bar{x_0} - \\bar{x}^* \\vert\\vert_A$, where $K(A) = \\frac{\\lambda_n}{\\lambda_1}$. This means that if we want the same precision, $10^{-6}$, as the conjugate gradient method using the steepest descent method we would have $\\sim \\sqrt{K(A)}$ more iterations. For example 75 iterations using the CG method whould be about 700 iterations using the SD method. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resources\n",
    "- [CG Method](https://towardsdatascience.com/complete-step-by-step-conjugate-gradient-algorithm-from-scratch-202c07fb52a8)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "fcc29b29288bfae7cd1f9163060093e50b801b38050887fcfbbbb4f16f6170ab"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}